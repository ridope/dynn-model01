{"cells":[{"cell_type":"code","source":["import torch\n","\n","gpu_avail = torch.cuda.is_available()\n","print(f\"Is the GPU available? {gpu_avail}\")\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(\"Device:\", device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPry8P68Dsb0","executionInfo":{"status":"ok","timestamp":1662382952571,"user_tz":-120,"elapsed":1857,"user":{"displayName":"ALEXANDRE SCHROEDER CHICORA","userId":"06799902252663641083"}},"outputId":"4c2798b5-64d5-4aba-d244-dcc3838c479c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Is the GPU available? True\n","Device: cuda\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"jOCQ0M07nrgR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662382981952,"user_tz":-120,"elapsed":26024,"user":{"displayName":"ALEXANDRE SCHROEDER CHICORA","userId":"06799902252663641083"}},"outputId":"45c86a6a-d578-4c51-8d86-55a70e461860"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 30.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n","Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.5.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (6.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ptflops\n","  Downloading ptflops-0.6.9.tar.gz (12 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from ptflops) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->ptflops) (4.1.1)\n","Building wheels for collected packages: ptflops\n","  Building wheel for ptflops (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ptflops: filename=ptflops-0.6.9-py3-none-any.whl size=11712 sha256=068e32f50c9560ac887da1c2838ccf27b49a1ddacb7ce557bf3fb867b96b1954\n","  Stored in directory: /root/.cache/pip/wheels/c8/71/2f/92426c1ef33fb2e275b533878d8378f91c7f26846d9669019c\n","Successfully built ptflops\n","Installing collected packages: ptflops\n","Successfully installed ptflops-0.6.9\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (0.18.3)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.6.3)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.7.3)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (7.1.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.9.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (3.2.2)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2021.11.2)\n","Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.21.6)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_msssim\n","  Downloading pytorch_msssim-0.2.1-py3-none-any.whl (7.2 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch_msssim) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch_msssim) (4.1.1)\n","Installing collected packages: pytorch-msssim\n","Successfully installed pytorch-msssim-0.2.1\n"]}],"source":["\n","!pip install tensorboardX\n","!pip install pyyaml\n","!pip install ptflops\n","!pip install tqdm\n","!pip install scikit-image\n","!pip install pytorch_msssim\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"VUl-HKkon8_f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662383014849,"user_tz":-120,"elapsed":25071,"user":{"displayName":"ALEXANDRE SCHROEDER CHICORA","userId":"06799902252663641083"}},"outputId":"ce859b90-39c6-42f0-8305-07303485d778"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"mLNg7egcinq1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662383018539,"user_tz":-120,"elapsed":1026,"user":{"displayName":"ALEXANDRE SCHROEDER CHICORA","userId":"06799902252663641083"}},"outputId":"50729652-9023-4877-8d91-a99c618461ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n","/content/drive/My Drive/Stage IETR/RDUNet-modifc\n","\u001b[0m\u001b[01;34mCheckpoints\u001b[0m/        LICENSE        \u001b[01;34m__pycache__\u001b[0m/         transforms.py\n","config.yaml         main_test.py   README.md            utils.py\n","data_management.py  main_train.py  \u001b[01;34mResults\u001b[0m/             val_files.txt\n","\u001b[01;34mDatasets\u001b[0m/           metrics.py     run-RDUSADNet.ipynb\n","\u001b[01;34mdcn\u001b[0m/                model.py       train_files.txt\n","\u001b[01;34mFigs\u001b[0m/               \u001b[01;34mPretrained\u001b[0m/    train.py\n"]}],"source":["%ls\n","%cd 'drive/My Drive/Stage IETR/RDUNet-modifc'\n","%ls"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"IXZ-x7G_Bpfi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662383022388,"user_tz":-120,"elapsed":403,"user":{"displayName":"ALEXANDRE SCHROEDER CHICORA","userId":"06799902252663641083"}},"outputId":"0273b6cc-af8b-42b9-b994-fda72c7a3eef"},"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","Tesla T4\n","Your runtime has 13.6 gigabytes of available RAM\n","\n","To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"\n","menu, and then select High-RAM in the Runtime shape dropdown. Then, \n","re-execute this cell.\n","Mon Sep  5 13:03:41 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   60C    P8    10W /  70W |      3MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2020 NVIDIA Corporation\n","Built on Mon_Oct_12_20:09:46_PDT_2020\n","Cuda compilation tools, release 11.1, V11.1.105\n","Build cuda_11.1.TC455_06.29190527_0\n","1.12.1+cu113\n"]}],"source":["print(torch.cuda.device_count())\n","print(torch.cuda.get_device_name(0))\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)\n","\n","!nvcc --version\n","print(torch.__version__)\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"NbUAlLpP4HP8"}},{"cell_type":"markdown","source":["# Models\n"],"metadata":{"id":"calAQsud4FCm"}},{"cell_type":"code","source":["#=============================CONV KERNEL======================================#\n","# torchvisison >= 0.9.0\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","from torchvision.ops import deform_conv2d\n","import math\n","import logging\n","logger = logging.getLogger('base')\n","\n","class DCN(nn.Module):\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 kernel_size=3,\n","                 stride=1,\n","                 padding=1,\n","                 dilation=1,\n","                 groups=1,\n","                 bias=True,\n","                 deformable_groups=1,\n","                 extra_offset_mask=True, \n","                 offset_in_channel=32\n","                 ):\n","        super(DCN, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.stride = stride \n","        self.padding = padding \n","        self.dilation = dilation\n","        self.groups = groups\n","        self.extra_offset_mask = extra_offset_mask\n","\n","        self.conv_offset_mask = nn.Conv2d(offset_in_channel,\n","                                     deformable_groups * 3 * kernel_size * kernel_size,\n","                                     kernel_size=kernel_size,\n","                                     stride=stride,\n","                                     padding=self.padding,\n","                                     bias=True)\n","\n","        self.init_offset()\n","\n","        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels // groups, kernel_size, kernel_size))\n","        if bias:\n","            self.bias = nn.Parameter(torch.Tensor(out_channels))\n","        else:\n","            self.bias = None\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        n = self.in_channels * self.kernel_size * self.kernel_size\n","        stdv = 1. / math.sqrt(n)\n","        self.weight.data.uniform_(-stdv, stdv)\n","        if self.bias is not None:\n","            self.bias.data.zero_()\n","    \n","    def init_offset(self):\n","        torch.nn.init.constant_(self.conv_offset_mask.weight, 0.)\n","        torch.nn.init.constant_(self.conv_offset_mask.bias, 0.)\n","\n","\n","    def forward(self, x):\n","\n","        if self.extra_offset_mask:\n","            # x = [input, features]\n","            offset_mask = self.conv_offset_mask(x[1])\n","            x = x[0]\n","        else:\n","            offset_mask = self.conv_offset_mask(x)\n","        o1, o2, mask = torch.chunk(offset_mask, 3, dim=1)\n","        offset = torch.cat((o1, o2), dim=1)\n","        mask = torch.sigmoid(mask)\n","\n","        offset_mean = torch.mean(torch.abs(offset))\n","        if offset_mean > max(x.shape[2:]):\n","            logger.warning('Offset mean is {}, larger than max(h, w).'.format(offset_mean))\n","\n","        out = deform_conv2d(input=x,\n","                            offset=offset,\n","                            weight=self.weight,\n","                            bias=self.bias,\n","                            stride=self.stride,\n","                            padding=self.padding,\n","                            dilation=self.dilation,\n","                            mask=mask\n","                            )\n","\n","\n","        return out\n","\n","\n","#=============================SADNET===========================================#\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.nn.functional as F\n","import numpy as np\n","from distutils.version import LooseVersion\n","import torchvision\n","\n","#==============================================================================#\n","class ResBlock(nn.Module):\n","\n","    def __init__(self, input_channel=32, output_channel=32):\n","        super().__init__()\n","        self.in_channel = input_channel\n","        self.out_channel = output_channel\n","        if self.in_channel != self.out_channel:\n","            self.conv0 = nn.Conv2d(input_channel, output_channel, 1, 1)\n","        self.conv1 = nn.Conv2d(output_channel, output_channel, 3, 1, 1)\n","        self.conv2 = nn.Conv2d(output_channel, output_channel, 3, 1, 1)\n","\n","        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        self.initialize_weights()\n","\n","    def forward(self, x):\n","        if self.in_channel != self.out_channel:\n","            x = self.conv0(x)\n","        conv1 = self.lrelu(self.conv1(x))\n","        conv2 = self.conv2(conv1)\n","        out = x + conv2\n","        return out\n","    def initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.xavier_uniform_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","\n","class RSABlock(nn.Module):\n","\n","    def __init__(self, input_channel=32, output_channel=32, offset_channel=32):\n","        super().__init__()\n","        self.in_channel = input_channel\n","        self.out_channel = output_channel\n","        if self.in_channel != self.out_channel:\n","            self.conv0 = nn.Conv2d(input_channel, output_channel, 1, 1)\n","        self.dcnpack = DCN(output_channel, output_channel, 3, stride=1, padding=1, dilation=1, deformable_groups=8,\n","                            extra_offset_mask=True, offset_in_channel=offset_channel)\n","        self.conv1 = nn.Conv2d(output_channel, output_channel, 3, 1, 1)\n","\n","        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        self.initialize_weights()\n","\n","    def forward(self, x, offset):\n","        if self.in_channel != self.out_channel:\n","            x = self.conv0(x)\n","        fea = self.lrelu(self.dcnpack([x, offset]))\n","        out = self.conv1(fea) + x\n","        return out\n","    def initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.xavier_uniform_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","\n","class OffsetBlock(nn.Module):\n","\n","    def __init__(self, input_channel=32, offset_channel=32, last_offset=False):\n","        super().__init__()\n","        self.offset_conv1 = nn.Conv2d(input_channel, offset_channel, 3, 1, 1)  # concat for diff\n","        if last_offset:\n","            self.offset_conv2 = nn.Conv2d(offset_channel*2, offset_channel, 3, 1, 1)  # concat for offset\n","        self.offset_conv3 = nn.Conv2d(offset_channel, offset_channel, 3, 1, 1)\n","\n","        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        self.initialize_weights()\n","\n","    def forward(self, x, last_offset=None):\n","        offset = self.lrelu(self.offset_conv1(x))\n","        if last_offset is not None:\n","            last_offset = F.interpolate(last_offset, scale_factor=2, mode='bilinear', align_corners=False)\n","            offset = self.lrelu(self.offset_conv2(torch.cat([offset, last_offset * 2], dim=1)))\n","        offset = self.lrelu(self.offset_conv3(offset))\n","        return offset\n","    def initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.xavier_uniform_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","\n","class ContextBlock(nn.Module):\n","    def __init__(self, input_channel=32, output_channel=32, square=False):\n","        super().__init__()\n","        self.conv0 = nn.Conv2d(input_channel, output_channel, 1, 1)\n","        if square:\n","            self.conv1 = nn.Conv2d(output_channel, output_channel, 3, 1, 1, 1)\n","            self.conv2 = nn.Conv2d(output_channel, output_channel, 3, 1, 2, 2)\n","            self.conv3 = nn.Conv2d(output_channel, output_channel, 3, 1, 4, 4)\n","            self.conv4 = nn.Conv2d(output_channel, output_channel, 3, 1, 8, 8)\n","        else:\n","            self.conv1 = nn.Conv2d(output_channel, output_channel, 3, 1, 1, 1)\n","            self.conv2 = nn.Conv2d(output_channel, output_channel, 3, 1, 2, 2)\n","            self.conv3 = nn.Conv2d(output_channel, output_channel, 3, 1, 3, 3)\n","            self.conv4 = nn.Conv2d(output_channel, output_channel, 3, 1, 4, 4)\n","        self.fusion = nn.Conv2d(4*output_channel, input_channel, 1, 1)\n","\n","        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        self.initialize_weights()\n","\n","    def forward(self, x):\n","        x_reduce = self.conv0(x)\n","        conv1 = self.lrelu(self.conv1(x_reduce))\n","        conv2 = self.lrelu(self.conv2(x_reduce))\n","        conv3 = self.lrelu(self.conv3(x_reduce))\n","        conv4 = self.lrelu(self.conv4(x_reduce))\n","        out = torch.cat([conv1, conv2, conv3, conv4], 1)\n","        out = self.fusion(out) + x\n","        return out\n","    def initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.xavier_uniform_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","\n","\n","#===============================================================================#\n","class SADNET(nn.Module):\n","\n","    def __init__(self, input_channel=3, output_channel=3, n_channel=128, offset_channel=128):\n","        super().__init__()\n","\n","        self.res1 = ResBlock(input_channel, n_channel)\n","        self.down1 = nn.Conv2d(n_channel, n_channel*2, 2, 2)\n","        self.res2 = ResBlock(n_channel*2, n_channel*2)\n","        self.down2 = nn.Conv2d(n_channel*2, n_channel*4, 2, 2)\n","        self.res3 = ResBlock(n_channel*4, n_channel*4)\n","        self.down3 = nn.Conv2d(n_channel*4, n_channel*8, 2, 2)\n","        self.res4 = ResBlock(n_channel*8, n_channel*8)\n","\n","        self.context = ContextBlock(n_channel*8, n_channel*2, square=False)\n","        self.offset4 = OffsetBlock(n_channel*8, offset_channel, False)\n","        self.dres4 = RSABlock(n_channel*8, n_channel*8, offset_channel)\n","\n","        self.up3 = nn.ConvTranspose2d(n_channel*8, n_channel*4, 2, 2)\n","        self.dconv3_1 = nn.Conv2d(n_channel*8, n_channel*4, 1, 1)\n","        self.offset3 = OffsetBlock(n_channel*4, offset_channel, True)\n","        self.dres3 = RSABlock(n_channel*4, n_channel*4, offset_channel)\n","\n","        self.up2 = nn.ConvTranspose2d(n_channel*4, n_channel*2, 2, 2)\n","        self.dconv2_1 = nn.Conv2d(n_channel*4, n_channel*2, 1, 1)\n","        self.offset2 = OffsetBlock(n_channel*2, offset_channel, True)\n","        self.dres2 = RSABlock(n_channel*2, n_channel*2, offset_channel)\n","\n","        self.up1 = nn.ConvTranspose2d(n_channel*2, n_channel, 2, 2)\n","        self.dconv1_1 = nn.Conv2d(n_channel*2, n_channel, 1, 1)\n","        self.offset1 = OffsetBlock(n_channel, offset_channel, True)\n","        self.dres1 = RSABlock(n_channel, n_channel, offset_channel)\n","\n","        self.out = nn.Conv2d(n_channel, output_channel, 3, 1, 1)\n","\n","        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","\n","    def forward(self, x):\n","        conv1 = self.res1(x)\n","        pool1 = self.lrelu(self.down1(conv1))\n","        conv2 = self.res2(pool1)\n","        pool2 = self.lrelu(self.down2(conv2))\n","        conv3 = self.res3(pool2)\n","        pool3 = self.lrelu(self.down3(conv3))\n","        conv4 = self.res4(pool3)\n","        conv4 = self.context(conv4)\n","\n","        L4_offset = self.offset4(conv4, None)\n","        dconv4 = self.dres4(conv4, L4_offset)\n","\n","        up3 = torch.cat([self.up3(dconv4), conv3], 1)\n","        up3 = self.dconv3_1(up3)\n","        L3_offset = self.offset3(up3, L4_offset)\n","        dconv3 = self.dres3(up3, L3_offset)\n","\n","        up2 = torch.cat([self.up2(dconv3), conv2], 1)\n","        up2 = self.dconv2_1(up2)\n","        L2_offset = self.offset2(up2, L3_offset)\n","        dconv2 = self.dres2(up2, L2_offset)\n","\n","        up1 = torch.cat([self.up1(dconv2), conv1], 1)\n","        up1 = self.dconv1_1(up1)\n","        L1_offset = self.offset1(up1, L2_offset)\n","        dconv1 = self.dres1(up1, L1_offset)\n","\n","        out = self.out(dconv1) + x\n","\n","        return out\n","\n","    def initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n","                #torch.nn.init.xavier_normal_(m.weight.data)\n","                torch.nn.init.xavier_uniform_(m.weight.data)\n","                #torch.nn.init.kaiming_uniform_(m.weight.data)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                torch.nn.init.normal_(m.weight.data, 0, 0.01)\n","                m.bias.data.zero_()\n","#==============================================================================#\n","\n","##########################################################\n","\n","###################### RDU NET ================================================\n","\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","@torch.no_grad()\n","def init_weights(init_type='xavier'):\n","    if init_type == 'xavier':\n","        init = nn.init.xavier_normal_\n","    elif init_type == 'he':\n","        init = nn.init.kaiming_normal_\n","    else:\n","        init = nn.init.orthogonal_\n","\n","    def initializer(m):\n","        classname = m.__class__.__name__\n","        if classname.find('Conv2d') != -1:\n","            init(m.weight)\n","        elif classname.find('BatchNorm') != -1:\n","            nn.init.normal_(m.weight, 1.0, 0.01)\n","            nn.init.zeros_(m.bias)\n","\n","    return initializer\n","\n","\n","class DownsampleBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DownsampleBlock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=2, stride=2)\n","        self.actv = nn.PReLU(out_channels)\n","\n","    def forward(self, x):\n","        return self.actv(self.conv(x))\n","\n","\n","class UpsampleBlock(nn.Module):\n","    def __init__(self, in_channels, cat_channels, out_channels):\n","        super(UpsampleBlock, self).__init__()\n","\n","        self.conv = nn.Conv2d(in_channels + cat_channels, out_channels, 3, padding=1)\n","        self.conv_t = nn.ConvTranspose2d(in_channels, in_channels, 2, stride=2)\n","        self.actv = nn.PReLU(out_channels)\n","        self.actv_t = nn.PReLU(in_channels)\n","\n","    def forward(self, x):\n","        upsample, concat = x\n","        upsample = self.actv_t(self.conv_t(upsample))\n","        return self.actv(self.conv(torch.cat([concat, upsample], 1)))\n","\n","\n","class InputBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(InputBlock, self).__init__()\n","        self.conv_1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n","        self.conv_2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n","\n","        self.actv_1 = nn.PReLU(out_channels)\n","        self.actv_2 = nn.PReLU(out_channels)\n","\n","    def forward(self, x):\n","        x = self.actv_1(self.conv_1(x))\n","        return self.actv_2(self.conv_2(x))\n","\n","\n","class OutputBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(OutputBlock, self).__init__()\n","        self.conv_1 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n","        self.conv_2 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n","\n","        self.actv_1 = nn.PReLU(in_channels)\n","        self.actv_2 = nn.PReLU(out_channels)\n","\n","    def forward(self, x):\n","        x = self.actv_1(self.conv_1(x))\n","        return self.actv_2(self.conv_2(x))\n","\n","\n","class DenoisingBlock(nn.Module):\n","    def __init__(self, in_channels, inner_channels, out_channels):\n","        super(DenoisingBlock, self).__init__()\n","        self.conv_0 = nn.Conv2d(in_channels, inner_channels, 3, padding=1)\n","        self.conv_1 = nn.Conv2d(in_channels + inner_channels, inner_channels, 3, padding=1)\n","        self.conv_2 = nn.Conv2d(in_channels + 2 * inner_channels, inner_channels, 3, padding=1)\n","        self.conv_3 = nn.Conv2d(in_channels + 3 * inner_channels, out_channels, 3, padding=1)\n","\n","        self.actv_0 = nn.PReLU(inner_channels)\n","        self.actv_1 = nn.PReLU(inner_channels)\n","        self.actv_2 = nn.PReLU(inner_channels)\n","        self.actv_3 = nn.PReLU(out_channels)\n","\n","    def forward(self, x):\n","        out_0 = self.actv_0(self.conv_0(x))\n","\n","        out_0 = torch.cat([x, out_0], 1)\n","        out_1 = self.actv_1(self.conv_1(out_0))\n","\n","        out_1 = torch.cat([out_0, out_1], 1)\n","        out_2 = self.actv_2(self.conv_2(out_1))\n","\n","        out_2 = torch.cat([out_1, out_2], 1)\n","        out_3 = self.actv_3(self.conv_3(out_2))\n","\n","        return out_3 + x\n","\n","\n","class DyNNet(nn.Module):\n","    \"\"\"\n","    Residual-Dense U-net for image denoising.\n","    \"\"\"\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","\n","        channels = 3\n","        filters_0 = 128\n","        filters_1 = 2 * filters_0\n","        filters_2 = 4 * filters_0\n","        filters_3 = 8 * filters_0\n","        offset_channel = filters_0\n","\n","        # Encoder:\n","        # Level 0:\n","        self.input_block = InputBlock(channels, filters_0)\n","        self.block_0_0 = DenoisingBlock(filters_0, filters_0 // 2, filters_0)\n","        self.block_0_1 = DenoisingBlock(filters_0, filters_0 // 2, filters_0)\n","        self.down_0 = DownsampleBlock(filters_0, filters_1)\n","\n","        # Level 1:\n","        self.block_1_0 = DenoisingBlock(filters_1, filters_1 // 2, filters_1)\n","        self.block_1_1 = DenoisingBlock(filters_1, filters_1 // 2, filters_1)\n","        self.down_1 = DownsampleBlock(filters_1, filters_2)\n","\n","        # Level 2:\n","        self.block_2_0 = DenoisingBlock(filters_2, filters_2 // 2, filters_2)\n","        self.block_2_1 = DenoisingBlock(filters_2, filters_2 // 2, filters_2)\n","        self.down_2 = DownsampleBlock(filters_2, filters_3)\n","\n","        # (Bottleneck)\n","        self.block_3_0 = DenoisingBlock(filters_3, filters_3 // 2, filters_3)\n","        self.block_3_1 = DenoisingBlock(filters_3, filters_3 // 2, filters_3)\n","        self.offset_3 = OffsetBlock(filters_3, offset_channel, False)\n","        self.rsab_3 = RSABlock(filters_3, filters_3, offset_channel)\n","\n","        # Decoder\n","        # Level 2:\n","        self.up_2 = UpsampleBlock(filters_3, filters_2, filters_2)\n","        self.block_2_2 = DenoisingBlock(filters_2, filters_2 // 2, filters_2)\n","        self.block_2_3 = DenoisingBlock(filters_2, filters_2 // 2, filters_2)\n","        self.offset_2 = OffsetBlock(filters_2, offset_channel, True)\n","        self.rsab_2 = RSABlock(filters_2, filters_2, offset_channel)\n","\n","        # Level 1:\n","        self.up_1 = UpsampleBlock(filters_2, filters_1, filters_1)\n","        self.block_1_2 = DenoisingBlock(filters_1, filters_1 // 2, filters_1)\n","        self.block_1_3 = DenoisingBlock(filters_1, filters_1 // 2, filters_1)\n","        self.offset_1 = OffsetBlock(filters_1, offset_channel, True)\n","        self.rsab_1 = RSABlock(filters_1, filters_1, offset_channel)\n","\n","        # Level 0:\n","        self.up_0 = UpsampleBlock(filters_1, filters_0, filters_0)\n","        self.block_0_2 = DenoisingBlock(filters_0, filters_0 // 2, filters_0)\n","        self.block_0_3 = DenoisingBlock(filters_0, filters_0 // 2, filters_0)\n","        self.offset_0 = OffsetBlock(filters_0, offset_channel, True)\n","        self.rsab_0 = RSABlock(filters_0, filters_0, offset_channel)\n","\n","        self.output_block = OutputBlock(filters_0, channels)\n","\n","    def forward(self, inputs):\n","        out_0 = self.input_block(inputs)    # Level 0\n","        out_0 = self.block_0_0(out_0)\n","        out_0 = self.block_0_1(out_0)\n","\n","        out_1 = self.down_0(out_0)          # Level 1\n","        out_1 = self.block_1_0(out_1)\n","        out_1 = self.block_1_1(out_1)\n","\n","        out_2 = self.down_1(out_1)          # Level 2\n","        out_2 = self.block_2_0(out_2)\n","        out_2 = self.block_2_1(out_2)\n","\n","        out_3 = self.down_2(out_2)          #(Bottleneck)\n","        out_3 = self.block_3_0(out_3)\n","        out_3 = self.block_3_1(out_3)      \n","        L3_offset = self.offset_3(out_3, None)\n","        dconv3 = self.rsab_3(out_3, L3_offset)\n","\n","        out_4 = self.up_2([dconv3, out_2])   # Level 2\n","        out_4 = self.block_2_2(out_4)\n","        out_4 = self.block_2_3(out_4)\n","        L2_offset = self.offset_2(out_4, L3_offset)\n","        dconv2 = self.rsab_2(out_4, L2_offset)\n","\n","        out_5 = self.up_1([dconv2, out_1])   # Level 1\n","        out_5 = self.block_1_2(out_5)\n","        out_5 = self.block_1_3(out_5)\n","        L1_offset = self.offset_1(out_5, L2_offset)\n","        dconv1 = self.rsab_1(out_5, L1_offset)\n","\n","        out_6 = self.up_0([dconv1, out_0])   # Level 0\n","        out_6 = self.block_0_2(out_6)\n","        out_6 = self.block_0_3(out_6)\n","        L0_offset = self.offset_0(out_6, L1_offset)\n","        dconv0 = self.rsab_0(out_6, L0_offset)\n","\n","        return self.output_block(dconv0) + inputs\n","\n"],"metadata":{"id":"pwogS64R4Ek1","executionInfo":{"status":"ok","timestamp":1662383345605,"user_tz":-120,"elapsed":489,"user":{"displayName":"ALEXANDRE SCHROEDER CHICORA","userId":"06799902252663641083"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Dependencies\n"],"metadata":{"id":"rCzB6vOv83xZ"}},{"cell_type":"code","source":["# Data managment\n","\n","import numpy as np\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, Sampler\n","from skimage.util import view_as_windows\n","\n","\n","\n","def data_augmentation(image):\n","    augmented_images_arrays, augmented_images_list = [], []\n","    to_transform = [image, np.rot90(image, axes=(1, 2))]\n","\n","    for t in to_transform:\n","        t_ud = t[:, ::-1, ...]\n","        t_lr = t[:, :, ::-1, ...]\n","        t_udlr = t_ud[:, :, ::-1, ...]\n","\n","        flips = [t_ud, t_lr, t_udlr]\n","        augmented_images_arrays.extend(flips)\n","\n","    augmented_images_arrays.extend(to_transform)\n","\n","    for img in augmented_images_arrays:\n","        img_unbatch = list(img)\n","        augmented_images_list.extend(img_unbatch)\n","\n","    return augmented_images_list\n","\n","\n","def create_patches(image, patch_size, step):\n","    image = view_as_windows(image, patch_size, step)\n","    h, w = image.shape[:2]\n","    image = np.reshape(image, (h * w, patch_size[0], patch_size[1], patch_size[2]))\n","\n","    return image\n","\n","\n","class DataSampler(Sampler):\n","\n","    def __init__(self, data_source, num_samples=None):\n","        super().__init__(data_source)\n","        self.data_source = data_source\n","        self._num_samples = num_samples\n","        self.rand = np.random.RandomState(0)\n","        self.perm = []\n","\n","    @property\n","    def num_samples(self):\n","        if self._num_samples is None:\n","            return len(self.data_source)\n","        return self._num_samples\n","\n","    def __iter__(self):\n","        n = len(self.data_source)\n","        if self._num_samples is not None:\n","            while len(self.perm) < self._num_samples:\n","                perm = self.rand.permutation(n).astype('int32').tolist()\n","                self.perm.extend(perm)\n","            idx = self.perm[:self._num_samples]\n","            self.perm = self.perm[self._num_samples:]\n","        else:\n","            idx = self.rand.permutation(n).astype('int32').tolist()\n","\n","        return iter(idx)\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","\n","class NoisyImagesDataset(Dataset):\n","    def __init__(self, files, channels, patch_size, transform=None, noise_transform=None):\n","        self.channels = channels\n","        self.patch_size = patch_size\n","        self.transform = transform\n","        self.noise_transforms = noise_transform\n","        self.to_tensor = ToTensor()\n","        self.dataset = {'image': [], 'noisy': []}\n","        self.load_dataset(files)\n","\n","    def __len__(self):\n","        return len(self.dataset['image'])\n","\n","    def __getitem__(self, idx):\n","        image, noisy = self.dataset.get('image')[idx], self.dataset.get('noisy')[idx]\n","        sample = {'image': image, 'noisy': noisy}\n","        if self.transform is not None:\n","            sample = self.transform(sample)\n","        sample = self.to_tensor(sample)\n","\n","        return sample.get('noisy'), sample.get('image')\n","\n","    def load_dataset(self, files):\n","        patch_size = (self.patch_size, self.patch_size, self.channels)\n","        for file in tqdm(files):\n","            image = load_image(file, self.channels)\n","            if image is None:\n","                continue\n","\n","            image = create_patches(image, patch_size, step=self.patch_size)\n","            sample = {'image': image, 'noisy': None}\n","\n","            for noise_transform in self.noise_transforms:\n","                _sample = noise_transform(sample)\n","                image, noisy = _sample['image'], _sample['noisy']\n","                image, noisy = list(image), list(noisy)\n","\n","                self.dataset['image'].extend(image)\n","                self.dataset['noisy'].extend(noisy)\n","\n","\n","# Train \n","\n","import csv\n","import os\n","import torch\n","import time\n","import numpy as np\n","from tqdm import tqdm\n","from torch import optim\n","\n","\n","class EpochLogger:\n","    r\"\"\"\n","    Keeps a log of metrics in the current epoch.\n","    \"\"\"\n","    def __init__(self):\n","        self.log = {\n","            'train loss': 0., 'train psnr': 0., 'train ssim': 0., 'val loss': 0., 'val psnr': 0., 'val ssim': 0.\n","        }\n","\n","    def update_log(self, metrics, phase):\n","        \"\"\"\n","        Update the metrics in the current epoch, this method is called at every step of the epoch.\n","        :param metrics: dict\n","            Metrics to update: loss, PSNR and SSIM.\n","        :param phase: str\n","            Phase of the current epoch: training (train) or validation (val).\n","        :return: None\n","        \"\"\"\n","        for key, value in metrics.items():\n","            self.log[' '.join([phase, key])] += value\n","\n","    def get_log(self, n_samples, phase):\n","        \"\"\"\n","        Returns the average of the monitored metrics in the current moment,\n","        given the number of evaluated samples.\n","        :param n_samples: int\n","            Number of evaluated samples.\n","        :param phase: str\n","            Phase of the current epoch: training (train) or validation (val).\n","        :return: dic\n","            Log of the current phase in the training.\n","        \"\"\"\n","        log = {\n","            phase + ' loss': self.log[phase + ' loss'] / n_samples,\n","            phase + ' psnr': self.log[phase + ' psnr'] / n_samples,\n","            phase + ' ssim': self.log[phase + ' ssim'] / n_samples\n","        }\n","        return log\n","\n","\n","class FileLogger(object):\n","    \"\"\"\n","    Keeps a log of the whole training and validation process.\n","    The results are recorded in a CSV files.\n","\n","    Args:\n","        file_path (string): path of the csv file.\n","    \"\"\"\n","    def __init__(self, file_path):\n","        \"\"\"\n","        Creates the csv record file.\n","        :param f\n","        \"\"\"\n","        self.file_path = file_path\n","        header = ['epoch', 'lr', 'train loss', 'train psnr', 'train ssim', 'val loss', 'val psnr', 'val ssim']\n","\n","        with open(self.file_path, 'w') as csv_file:\n","            file_writer = csv.writer(csv_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n","            file_writer.writerow(header)\n","\n","    def __call__(self, epoch_log):\n","        \"\"\"\n","        Updates the CSV record file.\n","        :param epoch_log: dict\n","            Log of the current epoch.\n","        :return: None\n","        \"\"\"\n","\n","        # Format log file:\n","        # Epoch and learning rate:\n","        log = ['{:03d}'.format(epoch_log['epoch']), '{:.5e}'.format(epoch_log['learning rate'])]\n","\n","        # Training loss, PSNR, SSIM:\n","        log.extend([\n","            '{:.5e}'.format(epoch_log['train loss']),\n","            '{:.5f}'.format(epoch_log['train psnr']),\n","            '{:.5f}'.format(epoch_log['train ssim'])\n","        ])\n","\n","        # Validation loss, PSNR, SSIM\n","        # Validation might not be done at all epochs, in that case the default calue is zero.\n","        log.extend([\n","            '{:.5e}'.format(epoch_log.get('val loss', 0.)),\n","            '{:.5f}'.format(epoch_log.get('val psnr', 0.)),\n","            '{:.5f}'.format(epoch_log.get('val ssim', 0.))\n","        ])\n","\n","        with open(self.file_path, 'a') as csv_file:\n","            file_writer = csv.writer(csv_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n","            file_writer.writerow(log)\n","\n","\n","def fit_model(model, data_loaders, channels, criterion, optimizer, scheduler, device, n_epochs, val_freq, checkpoint_dir, model_name):\n","    \"\"\"\n","    Training of the denoiser model.\n","    :param model: torch Module\n","        Neural network to fit.\n","    :param data_loaders: dict\n","        Dictionary with torch DataLoaders with training and validation datasets.\n","    :param channels: int\n","        Number of image channels\n","    :param criterion: torch Module\n","        Loss function.\n","    :param optimizer: torch Optimizer\n","        Gradient descent optimization algorithm.\n","    :param scheduler: torch lr_scheduler\n","        Learning rate scheduler.\n","    :param device: torch device\n","        Device used during training (CPU/GPU).\n","    :param n_epochs: int\n","        Number of epochs to fit the model.\n","    :param val_freq: int\n","        How many training epochs to run between validations.\n","    :param checkpoint_dir: str\n","        Path to the directory where the model checkpoints and CSV log files will be stored.\n","    :param model_name: str\n","        Prefix name of the trained model saved in checkpoint_dir.\n","    :return: None\n","    \"\"\"\n","    psnr = PSNR(data_range=1., reduction='sum')\n","    ssim = SSIM(channels, data_range=1., reduction='sum')\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","    logfile_path = os.path.join(checkpoint_dir,  ''.join([model_name, '_logfile.csv']))\n","    model_path = os.path.join(checkpoint_dir, ''.join([model_name, '-{:03d}-{:.4e}-{:.4f}-{:.4f}.pth']))\n","    file_logger = FileLogger(logfile_path)\n","    best_model_path, best_psnr = '', -np.inf\n","    since = time.time()\n","\n","    for epoch in range(1, n_epochs + 1):\n","        lr = optimizer.param_groups[0]['lr']\n","        epoch_logger = EpochLogger()\n","        epoch_log = dict()\n","\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()\n","                print('\\nEpoch: {}/{} - Learning rate: {:.4e}'.format(epoch, n_epochs, lr))\n","                description = 'Training - Loss:{:.5e} - PSNR:{:.5f} - SSIM:{:.5f}'\n","            elif phase == 'val' and epoch % val_freq == 0:\n","                model.eval()\n","                description = 'Validation - Loss:{:.5e} - PSNR:{:.5f} - SSIM:{:.5f}'\n","            else:\n","                break\n","\n","            iterator = tqdm(enumerate(data_loaders[phase], 1), total=len(data_loaders[phase]), ncols=110)\n","            iterator.set_description(description.format(0, 0, 0))\n","            n_samples = 0\n","\n","            for step, (inputs, targets) in iterator:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, targets)\n","\n","                if phase == 'train':\n","                    loss.backward()\n","                    optimizer.step()\n","\n","                n_samples += inputs.size()[0]\n","                metrics = {\n","                    'loss': loss.item() * inputs.size()[0],\n","                    'psnr': psnr(outputs, targets).item(),\n","                    'ssim': ssim(outputs, targets).item()\n","                }\n","                epoch_logger.update_log(metrics, phase)\n","                log = epoch_logger.get_log(n_samples, phase)\n","                iterator.set_description(description.format(log[phase + ' loss'], log[phase + ' psnr'], log[phase + ' ssim']))\n","\n","            if phase == 'val':\n","                # Apply Reduce LR On Plateau if it is the case and save the model if the validation PSNR is improved.\n","                if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n","                    scheduler.step(log['val psnr'])\n","                if log['val psnr'] > best_psnr:\n","                    best_psnr = log['val psnr']\n","                    best_model_path = model_path.format(epoch, log['val loss'], log['val psnr'], log['val ssim'])\n","                    torch.save(model.state_dict(), best_model_path)\n","\n","            elif scheduler is not None:         # Apply another scheduler at epoch level.\n","                scheduler.step()\n","\n","            epoch_log = {**epoch_log, **log}\n","\n","        # Save the current epoch metrics in a CVS file.\n","        epoch_data = {'epoch': epoch, 'learning rate': lr, **epoch_log}\n","        file_logger(epoch_data)\n","\n","    # Save the last model and report training time.\n","    best_model_path = model_path.format(epoch, log['val loss'], log['val psnr'], log['val ssim'])\n","    torch.save(model.state_dict(), best_model_path)\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best PSNR: {:4f}'.format(best_psnr))\n","\n","\n","# Metrics\n","\n","import torch\n","from pytorch_msssim import SSIM as _SSIM\n","\n","\n","class PSNR(object):\n","    r\"\"\"\n","    Evaluates the PSNR metric in a tensor.\n","    It can return a result with different reduction methods.\n","\n","    Args:\n","        data_range (int, float): Range of the input images.\n","        reduction (string): Specifies the reduction to apply to the output:\n","            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n","            ``'mean'``: the sum of the output will be divided by the number of\n","            elements in the output, ``'sum'``: the output will be summed.\n","        eps (float): Epsilon value to avoid division by zero.\n","    \"\"\"\n","    def __init__(self, data_range, reduction='none', eps=1e-8):\n","        self.data_range = data_range\n","        self.reduction = reduction\n","        self.eps = eps\n","\n","    def __call__(self, outputs, targets):\n","        with torch.set_grad_enabled(False):\n","            mse = torch.mean((outputs - targets) ** 2., dim=(1, 2, 3))\n","            psnr = 10. * torch.log10((self.data_range ** 2.) / (mse + self.eps))\n","\n","            if self.reduction == 'mean':\n","                return psnr.mean()\n","            if self.reduction == 'sum':\n","                return psnr.sum()\n","\n","            return psnr\n","\n","\n","class SSIM(object):\n","    r\"\"\"\n","    Evaluates the SSIM metric in a tensor.\n","    It can return a result with different reduction methods.\n","\n","    Args:\n","        channels (int): Number of channels of the images.\n","        data_range (int, float): Range of the input images.\n","        reduction (string): Specifies the reduction to apply to the output:\n","            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n","            ``'mean'``: the sum of the output will be divided by the number of\n","            elements in the output, ``'sum'``: the output will be summed.\n","    \"\"\"\n","    def __init__(self, channels, data_range, reduction='none'):\n","        self.data_range = data_range\n","        self.reduction = reduction\n","        self.ssim_module = _SSIM(data_range=data_range, size_average=False, channel=channels)\n","\n","    def __call__(self, outputs, targets):\n","        with torch.set_grad_enabled(False):\n","            ssim = self.ssim_module(outputs, targets)\n","\n","            if self.reduction == 'mean':\n","                return ssim.mean()\n","            if self.reduction == 'sum':\n","                return ssim.sum()\n","\n","            return ssim\n","\n","\n","# Transforms\n","\n","import random\n","import torch\n","import numpy as np\n","\n","\n","class AdditiveWhiteGaussianNoise(object):\n","    \"\"\"Additive white gaussian noise generator.\"\"\"\n","    def __init__(self, noise_level, fix_sigma=False, clip=False):\n","        self.noise_level = noise_level\n","        self.fix_sigma = fix_sigma\n","        self.rand = np.random.RandomState(1)\n","        self.clip = clip\n","        if not fix_sigma:\n","            self.predefined_noise = [i for i in range(5, noise_level + 1, 5)]\n","\n","    def __call__(self, sample):\n","        \"\"\"\n","        Generates additive white gaussian noise, and it is applied to the clean image.\n","        :param sample:\n","        :return:\n","        \"\"\"\n","        image = sample.get('image')\n","\n","        if image.ndim == 4:                 # if 'image' is a batch of images, we set a different noise level per image\n","            samples = image.shape[0]        # (Samples, Height, Width, Channels) or (Samples, Channels, Height, Width)\n","            if self.fix_sigma:\n","                sigma = self.noise_level * np.ones((samples, 1, 1, 1))\n","            else:\n","                sigma = np.random.choice(self.predefined_noise, size=(samples, 1, 1, 1))\n","            noise = self.rand.normal(0., 1., size=image.shape)\n","            noise = noise * sigma\n","        else:                               # else, 'image' is a simple image\n","            if self.fix_sigma:              # (Height, Width, Channels) or (Channels , Height, Width)\n","                sigma = self.noise_level\n","            else:\n","                sigma = self.rand.randint(5, self.noise_level)\n","            noise = self.rand.normal(0., sigma, size=image.shape)\n","\n","        noisy = image + noise\n","\n","        if self.clip:\n","            noisy = np.clip(noisy, 0., 255.)\n","\n","        return {'image': image, 'noisy': noisy.astype('float32')}\n","\n","\n","class ToTensor(object):\n","    \"\"\"Convert data sample to pytorch tensor\"\"\"\n","    def __call__(self, sample):\n","        image, noisy = sample.get('image'), sample.get('noisy')\n","        image = torch.from_numpy(image.transpose((2, 0, 1)).astype('float32') / 255.)\n","\n","        if noisy is not None:\n","            noisy = torch.from_numpy(noisy.transpose((2, 0, 1)).astype('float32') / 255.)\n","\n","        return {'image': image, 'noisy': noisy}\n","\n","\n","class RandomVerticalFlip(object):\n","\n","    def __init__(self, p=0.5):\n","        self.p = p\n","\n","    def __call__(self, sample):\n","        if random.uniform(0., 1.) < self.p:\n","            image, noisy = sample.get('image'), sample.get('noisy')\n","            image = np.flipud(image)\n","\n","            if noisy is not None:\n","                noisy = np.flipud(noisy)\n","\n","            return {'image': image, 'noisy': noisy}\n","\n","        return sample\n","\n","\n","class RandomHorizontalFlip(object):\n","\n","    def __init__(self, p=0.5):\n","        self.p = p\n","\n","    def __call__(self, sample):\n","        if random.uniform(0., 1.) < self.p:\n","            image, noisy = sample.get('image'), sample.get('noisy')\n","            image = np.fliplr(image)\n","\n","            if noisy is not None:\n","                noisy = np.fliplr(noisy)\n","\n","            return {'image': image, 'noisy': noisy}\n","\n","        return sample\n","\n","\n","class RandomRot90(object):\n","\n","    def __init__(self, p=0.5):\n","        self.p = p\n","\n","    def __call__(self, sample):\n","        if random.uniform(0., 1.) < self.p:\n","            image, noisy = sample.get('image'), sample.get('noisy')\n","            image = np.rot90(image)\n","\n","            if noisy is not None:\n","                noisy = np.rot90(noisy)\n","\n","            return {'image': image, 'noisy': noisy}\n","\n","        return sample\n","\n","\n","# Utils\n","\n","import random\n","import torch\n","import numpy as np\n","from skimage import io, color, img_as_ubyte\n","\n","\n","def load_image(image_path, channels):\n","    \"\"\"\n","    Load image and change it color space from RGB to Grayscale if necessary.\n","    :param image_path: str\n","        Path of the image.\n","    :param channels: int\n","        Number of channels (3 for RGB, 1 for Grayscale)\n","    :return: numpy array\n","        Image loaded.\n","    \"\"\"\n","    image = io.imread(image_path)\n","\n","    if image.ndim == 3 and channels == 1:       # Convert from RGB to Grayscale and expand dims.\n","        image = img_as_ubyte(color.rgb2gray(image))\n","        return np.expand_dims(image, axis=-1)\n","    elif image.ndim == 2 and channels == 1:     # Handling grayscale images if needed.\n","        if image.dtype != 'uint8':\n","            image = img_as_ubyte(image)\n","        return np.expand_dims(image, axis=-1)\n","\n","    return image\n","\n","\n","def mod_crop(image, mod):\n","    \"\"\"\n","    Crops image according to mod to restore spatial dimensions\n","    adequately in the decoding sections of the model.\n","    :param image: numpy array\n","        Image to crop.\n","    :param mod: int\n","        Module for padding allowed by the number of\n","        encoding/decoding sections in the model.\n","    :return: numpy array\n","        Copped image\n","    \"\"\"\n","    size = image.shape[:2]\n","    size = size - np.mod(size, mod)\n","    image = image[:size[0], :size[1], ...]\n","\n","    return image\n","\n","\n","def mod_pad(image, mod):\n","    \"\"\"\n","    Pads image according to mod to restore spatial dimensions\n","    adequately in the decoding sections of the model.\n","    :param image: numpy array\n","        Image to pad.\n","    :param mod: int\n","        Module for padding allowed by the number of\n","        encoding/decoding sections in the model.\n","    :return: numpy  array, tuple\n","        Padded image, original image size.\n","    \"\"\"\n","    size = image.shape[:2]\n","    h, w = np.mod(size, mod)\n","    h, w = mod - h, mod - w\n","    if h != mod or w != mod:\n","        if image.ndim == 3:\n","            image = np.pad(image, ((0, h), (0, w), (0, 0)), mode='reflect')\n","        else:\n","            image = np.pad(image, ((0, h), (0, w)), mode='reflect')\n","\n","    return image, size\n","\n","\n","def set_seed(seed=1):\n","    \"\"\"\n","    Sets all random seeds.\n","    :param seed: int\n","        Seed value.\n","    :return: None\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","\n","def build_ensemble(image, normalize=True):\n","    \"\"\"\n","    Create image ensemble to estimate denoised image.\n","    :param image: numpy array\n","        Noisy image.\n","    :param normalize: bool\n","        Normalize image to range [0., 1.].\n","    :return: list\n","        Ensemble of noisy image transformed.\n","    \"\"\"\n","    img_rot = np.rot90(image)\n","    ensemble_list = [\n","        image, np.fliplr(image), np.flipud(image), np.flipud(np.fliplr(image)),\n","        img_rot, np.fliplr(img_rot), np.flipud(img_rot), np.flipud(np.fliplr(img_rot))\n","    ]\n","\n","    ensemble_transformed = []\n","    for img in ensemble_list:\n","        if img.ndim == 2:                                           # Expand dims for channel dimension in gray scale.\n","            img = np.expand_dims(img.copy(), 0)                     # Use copy to avoid problems with reverse indexing.\n","        else:\n","            img = np.transpose(img.copy(), (2, 0, 1))               # Channels-first transposition.\n","        if normalize:\n","            img = img / 255.\n","\n","        img_t = torch.from_numpy(np.expand_dims(img, 0)).float()    # Expand dims again to create batch dimension.\n","        ensemble_transformed.append(img_t)\n","\n","    return ensemble_transformed\n","\n","\n","def separate_ensemble(ensemble, return_single=False):\n","    \"\"\"\n","    Apply inverse transforms to predicted image ensemble and average them.\n","    :param ensemble: list\n","        Predicted images, ensemble[0] is the original image,\n","        and ensemble[i] is a transformed version of ensemble[i].\n","    :param return_single: bool\n","        Return also ensemble[0] to evaluate single prediction\n","    :return: numpy array or tuple of numpy arrays\n","        Average of the predicted images, original image denoised.\n","    \"\"\"\n","    ensemble_np = []\n","\n","    for img in ensemble:\n","        img = img.squeeze()                     # Remove additional dimensions.\n","        if img.ndim == 3:                       # Transpose if necessary.\n","            img = np.transpose(img, (1, 2, 0))\n","\n","        ensemble_np.append(img)\n","\n","    # Apply inverse transforms to vertical and horizontal flips.\n","    img = ensemble_np[0] + np.fliplr(ensemble_np[1]) + np.flipud(ensemble_np[2]) + np.fliplr(np.flipud(ensemble_np[3]))\n","\n","    # Apply inverse transforms to 90º rotation, vertical and horizontal flips\n","    img = img + np.rot90(ensemble_np[4], k=3) + np.rot90(np.fliplr(ensemble_np[5]), k=3)\n","    img = img + np.rot90(np.flipud(ensemble_np[6]), k=3) + np.rot90(np.fliplr(np.flipud(ensemble_np[7])), k=3)\n","\n","    # Average and clip final predicted image.\n","    img = img / 8.\n","    img = np.clip(img, 0., 1.)\n","\n","    if return_single:\n","        return img, ensemble_np[0]\n","    else:\n","        return img\n","\n","\n","def predict_ensemble(model, ensemble, device):\n","    \"\"\"\n","    Predict batch of images from an ensemble.\n","    :param model: torch Module\n","        Trained model to estimate denoised images.\n","    :param ensemble: list\n","        Images to estimate.\n","    :param device: torch device\n","        Device of the trained model.\n","    :return: list\n","        Estimated images of type numpy ndarray.\n","    \"\"\"\n","    y_hat_ensemble = []\n","\n","    for x in ensemble:\n","        x = x.to(device)\n","\n","        with torch.no_grad():\n","            y_hat = model(x)\n","            y_hat_ensemble.append(y_hat.cpu().detach().numpy().astype('float32'))\n","\n","    return y_hat_ensemble\n","\n"],"metadata":{"id":"FPGUCNGe83UC","executionInfo":{"status":"ok","timestamp":1662383089828,"user_tz":-120,"elapsed":400,"user":{"displayName":"ALEXANDRE SCHROEDER CHICORA","userId":"06799902252663641083"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"cjkFYJ-RdKde"}},{"cell_type":"code","source":["print(\"Device:\", device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l53KdAf1_Pih","executionInfo":{"status":"ok","timestamp":1662041755192,"user_tz":-120,"elapsed":708,"user":{"displayName":"ALEXANDRE SCHROEDER CHICORA","userId":"06799902252663641083"}},"outputId":"31068872-b49a-4dbe-e307-3122f94d848f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}]},{"cell_type":"markdown","source":["# Training code\n"],"metadata":{"id":"N8qBoGL5auJ4"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"bY9GasUc1MK9","colab":{"base_uri":"https://localhost:8080/","height":382},"outputId":"cc9e6f94-fc10-48ff-9d16-39ebab0e22de","executionInfo":{"status":"error","timestamp":1662391407696,"user_tz":-120,"elapsed":3332,"user":{"displayName":"ALEXANDRE SCHROEDER CHICORA","userId":"06799902252663641083"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Training - Loss:9.17055e-03 - PSNR:39.39477 - SSIM:0.95999:  48%|█████▎     | 130/272 [03:35<03:54,  1.65s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-7b2d029c2a70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-7b2d029c2a70>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     fit_model(model, data_loaders, model_params['channels'], criterion, optimizer, lr_scheduler, device,\n\u001b[0;32m--> 102\u001b[0;31m               n_epochs, val_params['frequency'], train_params['checkpoint path'], model_name)\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-1555c549a9c8>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, data_loaders, channels, criterion, optimizer, scheduler, device, n_epochs, val_freq, checkpoint_dir, model_name)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import yaml\n","import torch\n","import torch.optim as optim\n","from os.path import join\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import transforms\n","from ptflops import get_model_complexity_info\n","\n","\n","def main():\n","    with open('config.yaml', 'r') as stream:                # Load YAML configuration file.\n","        config = yaml.safe_load(stream)\n","\n","    model_params = config['model']\n","    train_params = config['train']\n","    val_params = config['val']\n","\n","    # Defining model:\n","    set_seed(0)\n","    model = DyNNet()\n","\n","    print('Model summary:')\n","    test_shape = (model_params['channels'], train_params['patch size'], train_params['patch size'])\n","    with torch.no_grad():\n","        macs, params = get_model_complexity_info(model, test_shape, as_strings=True,\n","                                                 print_per_layer_stat=False, verbose=False)\n","        print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n","        print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n","\n","    # Define the model name and use multi-GPU if it is allowed.\n","    model_name = 'model_color' if model_params['channels'] == 3 else 'model_gray'\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","    device = torch.device(train_params['device'])\n","    print(\"Using device: {}\".format(device))\n","    #if torch.cuda.device_count() > 1 and 'cuda' in device.type and train_params['multi gpu']:\n","    #    model = nn.DataParallel(model)\n","    #    print('Using multiple GPUs')\n","\n","    model = model.to(device)\n","    param_group = []\n","    for name, param in model.named_parameters():\n","        if 'conv' in name and 'weight' in name:\n","            p = {'params': param, 'weight_decay': train_params['weight decay']}\n","        else:\n","            p = {'params': param, 'weight_decay': 0.}\n","        param_group.append(p)\n","\n","    # Load training and validation file names.\n","    # Modify .txt files if datasets do not fit in memory.\n","    with open('train_files.txt', 'r') as f_train, open('val_files.txt', 'r') as f_val:\n","        raw_train_files = f_train.read().splitlines()\n","        raw_val_files = f_val.read().splitlines()\n","        train_files = list(map(lambda file: join(train_params['dataset path'], file), raw_train_files))\n","        val_files = list(map(lambda file: join(val_params['dataset path'], file), raw_val_files))\n","\n","    training_transforms = transforms.Compose([\n","        RandomHorizontalFlip(),\n","        RandomVerticalFlip(),\n","        RandomRot90()\n","    ])\n","\n","    # Predefined noise level\n","    train_noise_transform = [AdditiveWhiteGaussianNoise(train_params['noise level'], clip=True)]\n","    val_noise_transforms = [AdditiveWhiteGaussianNoise(s, fix_sigma=True, clip=True) for s in val_params['noise levels']]\n","\n","    print('\\nLoading training dataset:')\n","    training_dataset = NoisyImagesDataset(train_files,\n","                                          model_params['channels'],\n","                                          train_params['patch size'],\n","                                          training_transforms,\n","                                          train_noise_transform)\n","\n","    print('\\nLoading validation dataset:')\n","    validation_dataset = NoisyImagesDataset(val_files,\n","                                            model_params['channels'],\n","                                            val_params['patch size'],\n","                                            None,\n","                                            val_noise_transforms)\n","    # Training in sub-epochs:\n","    print('Training patches:', len(training_dataset))\n","    print('Validation patches:', len(validation_dataset))\n","    n_samples = len(training_dataset) // train_params['dataset splits']\n","    n_epochs = train_params['epochs'] * train_params['dataset splits']\n","    sampler = DataSampler(training_dataset, num_samples=n_samples)\n","\n","    data_loaders = {\n","        'train': DataLoader(training_dataset, train_params['batch size'], num_workers=train_params['workers'], sampler=sampler),\n","        'val': DataLoader(validation_dataset, val_params['batch size'], num_workers=val_params['workers']),\n","    }\n","\n","    # Optimization:\n","    learning_rate = train_params['learning rate']\n","    step_size = train_params['scheduler step'] * train_params['dataset splits']\n","\n","    criterion = nn.L1Loss()\n","    optimizer = optim.AdamW(param_group, lr=learning_rate)\n","    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=train_params['scheduler gamma'])\n","\n","    # Train the model\n","    fit_model(model, data_loaders, model_params['channels'], criterion, optimizer, lr_scheduler, device,\n","              n_epochs, val_params['frequency'], train_params['checkpoint path'], model_name)\n","\n","\n","if __name__ == '__main__':\n","    main()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y-iXcPnU0QfU"},"source":[]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"nOCgi2xBDVYd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlRdsFyI3A18","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662108864153,"user_tz":-120,"elapsed":84564,"user":{"displayName":"ALEXANDRE SCHROEDER CHICORA","userId":"06799902252663641083"}},"outputId":"53f0ba9f-c123-430a-fae0-2d5db602ca29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Dataset:  noisy_cbsd68_\n","Image: 1 - PSNR: 25.3512 - SSIM: 0.6324 - ens PSNR: 25.1547 - ens SSIM: 0.6255\n","Image: 2 - PSNR: 26.4824 - SSIM: 0.5781 - ens PSNR: 26.5818 - ens SSIM: 0.5826\n","Image: 3 - PSNR: 26.1376 - SSIM: 0.5629 - ens PSNR: 26.1642 - ens SSIM: 0.5637\n","Image: 4 - PSNR: 26.0297 - SSIM: 0.6865 - ens PSNR: 26.0448 - ens SSIM: 0.6905\n","Image: 5 - PSNR: 26.9086 - SSIM: 0.5289 - ens PSNR: 26.9940 - ens SSIM: 0.5331\n","sigma = 30 - PSNR: 26.1819 - SSIM: 0.5978 - ens PSNR: 26.1879 - ens SSIM: 0.5991\n","Image: 1 - PSNR: 20.7418 - SSIM: 0.3934 - ens PSNR: 20.6822 - ens SSIM: 0.3888\n","Image: 2 - PSNR: 21.7593 - SSIM: 0.3277 - ens PSNR: 21.8136 - ens SSIM: 0.3304\n","Image: 3 - PSNR: 21.3582 - SSIM: 0.3154 - ens PSNR: 21.3928 - ens SSIM: 0.3161\n","Image: 4 - PSNR: 21.2530 - SSIM: 0.4094 - ens PSNR: 21.2681 - ens SSIM: 0.4108\n","Image: 5 - PSNR: 21.8176 - SSIM: 0.2672 - ens PSNR: 21.8765 - ens SSIM: 0.2693\n","sigma = 50 - PSNR: 21.3860 - SSIM: 0.3426 - ens PSNR: 21.4067 - ens SSIM: 0.3431\n"]}],"source":["import os\n","import yaml\n","import torch\n","import numpy as np\n","import scipy.io as sio\n","from os.path import join\n","from skimage import io\n","from skimage.metrics import structural_similarity, peak_signal_noise_ratio\n","\n","\n","def predict(model, noisy_dataset, gt_dataset, device, padding, n_channels, results_path):\n","    # Load test datasets in format .mat\n","    X = sio.loadmat(noisy_dataset)['data'].flatten()\n","    Y = sio.loadmat(gt_dataset)['label'].flatten()\n","\n","    y_pred, y_pred_ens = [], []\n","    psnr_list, ssim_list = [], []\n","    ens_psnr_list, ens_ssim_list = [], []\n","\n","    n_images = len(X)\n","    multi_channel = True if n_channels == 3 else False\n","\n","    for i in range(n_images):\n","        x, y = X[i], Y[i]\n","\n","        if padding:\n","            x, size = mod_pad(x, 8)\n","        else:\n","            x, y = mod_crop(x, 8), mod_crop(y, 8)\n","\n","        x = build_ensemble(x, normalize=False)\n","\n","        with torch.no_grad():\n","            y_hat_ens = predict_ensemble(model, x, device)\n","            y_hat_ens, y_hat = separate_ensemble(y_hat_ens, return_single=True)\n","\n","            if padding:\n","                y_hat = y_hat[:size[0], :size[1], ...]\n","                y_hat_ens = y_hat_ens[:size[0], :size[1], ...]\n","\n","            y_pred.append(y_hat)\n","            y_pred_ens.append(y_hat_ens)\n","            psnr = peak_signal_noise_ratio(y, y_hat, data_range=1.)\n","            ssim = structural_similarity(y, y_hat, data_range=1., multichannel=multi_channel, gaussian_weights=True,\n","                                         sigma=1.5, use_sample_covariance=False)\n","\n","            psnr_ens = peak_signal_noise_ratio(y, y_hat_ens, data_range=1.)\n","            ssim_ens = structural_similarity(y, y_hat_ens, data_range=1., multichannel=multi_channel,\n","                                             gaussian_weights=True, sigma=1.5, use_sample_covariance=False)\n","\n","            psnr_list.append(psnr)\n","            ssim_list.append(ssim)\n","\n","            ens_psnr_list.append(psnr_ens)\n","            ens_ssim_list.append(ssim_ens)\n","            print('Image: {} - PSNR: {:.4f} - SSIM: {:.4f} - ens PSNR: {:.4f}'\n","                  ' - ens SSIM: {:.4f}'.format(i + 1, psnr, ssim, psnr_ens, ssim_ens))\n","\n","    if results_path is not None:\n","        for i in range(n_images):\n","            y_hat = (255 * y_pred[i]).astype('uint8')\n","            y_hat_ens = (255 * y_pred_ens[i]).astype('uint8')\n","\n","            y_hat = np.squeeze(y_hat)\n","            y_hat_ens = np.squeeze(y_hat_ens)\n","\n","            os.makedirs(results_path, exist_ok=True)\n","\n","            name = os.path.join(results_path, '{}_{:.4f}_{:.4f}.png'.format(i, psnr_list[i], ssim_list[i]))\n","            io.imsave(name, y_hat)\n","\n","            name = os.path.join(results_path, '{}_{:.4f}_{:.4f}_ens.png'.format(i, ens_psnr_list[i], ens_ssim_list[i]))\n","            io.imsave(name, y_hat_ens)\n","\n","    return np.mean(psnr_list), np.mean(ssim_list), np.mean(ens_psnr_list), np.mean(ens_ssim_list)\n","\n","\n","if __name__ == '__main__':\n","    with open('config.yaml', 'r') as stream:\n","        config = yaml.safe_load(stream)\n","\n","    model_params = config['model']\n","    test_params = config['test']\n","    n_channels = model_params['channels']\n","\n","    \n","    model_path = join(test_params['pretrained models path'], 'model_color.pth')\n","    noisy_datasets = ['noisy_cbsd68_']  # Also tested in Kodak24 and Urban100 datasets.\n","    gt_datasets = ['cbsd68_label']\n","\n","\n","    model_params = config['model']\n","    model = SADNET()\n","\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","    print(\"Using device: {}\".format(device))\n","\n","    state_dict = torch.load(model_path)\n","    model.load_state_dict(state_dict, strict=True)\n","    model = model.to(device)\n","    model.eval()\n","\n","    for noisy_dataset, gt_dataset in zip(noisy_datasets, gt_datasets):\n","        print('Dataset: ', noisy_dataset)\n","\n","        for noise_level in test_params['noise levels']:\n","            extension = '_color' if model_params['channels'] == 3 else '_gray'\n","\n","            noisy_path = join(test_params['dataset path'], ''.join([noisy_dataset, str(noise_level), extension, '.mat']))\n","            label_path = join(test_params['dataset path'], ''.join([gt_dataset, extension, '.mat']))\n","\n","            if test_params['save images']:\n","                save_path = join(\n","                    test_params['results path'], ''.join([noisy_dataset.replace('noisy_', ''), 'sigma_', str(noise_level)])\n","                )\n","            else:\n","                save_path = None\n","\n","            psnr, ssim, psnr_ens, ssim_ens = predict(model, noisy_path, label_path, device,\n","                                                     test_params['padding'],  n_channels, save_path)\n","\n","            message = 'sigma = {} - PSNR: {:.4f} - SSIM: {:.4f} - ens PSNR: {:.4f} - ens SSIM: {:.4f}'\n","            print(message.format(noise_level, np.around(psnr, decimals=4), np.around(ssim, decimals=4),\n","                                 np.around(psnr_ens, decimals=4), np.around(ssim_ens, decimals=4)))\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyOvOzAQUCktsRIwfQ4F34Hw"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}